{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指南 2：使用 PyTorch 研究项目\n",
    "\n",
    "* 根据我得到的一些反馈，我们将尝试总结有关如何在 PyTorch 中设置和构建大型研究项目的提示和技巧，例如您的硕士论文\n",
    "* 如果你有好的想法，请随意贡献自己\n",
    "\n",
    "## 设置\n",
    "\n",
    "### 框架\n",
    "\n",
    "* 选择正确的框架至关重要。 如果您采用标准的循环优化单个前向传递及其损失，请考虑使用 PyTorch Lightning。 它大大减少了代码开销，并允许在需要时轻松地将模型扩展到多个 GPU 和/或节点。 不过，如果您预计需要对默认训练过程进行相当大的更改，请考虑使用普通 PyTorch 并编写您自己的框架。 最初可能需要更多时间，但可以更轻松地在优化过程中进行编辑。\n",
    "* 对于自己的框架，以下可用作示例设置：\n",
    "\n",
    "```bash\n",
    "general/\n",
    "│   train.py\n",
    "│   task.py\n",
    "│   mutils.py\n",
    "layers/\n",
    "experiments/\n",
    "│   task1/\n",
    "│        train.py\n",
    "│        task.py\n",
    "│        eval.py\n",
    "│        dataset.py\n",
    "│   task2/\n",
    "│        train.py\n",
    "│        task.py\n",
    "│        eval.py\n",
    "│        dataset.py\n",
    "```\n",
    "\n",
    "* `general/train.py` 文件涵盖了每个模型需要的默认操作（训练循环、加载/保存模型、设置模型等）。 如果您使用 PyTorch Lightning，这会减少每个任务的训练代码文件，并且只需要设计训练对象。\n",
    "* `general/task.py` 文件涵盖了您必须为任务执行的特定部分（训练步骤、验证步骤等）的模板。 如果您使用 PyTorch Lightning，这将会在 Lightning 模块的定义。\n",
    "* `layers/models` 文件夹包含用于指定用于设置模型的 `nn.Modules` 的代码。\n",
    "* `experiments` 文件夹包含特定于任务的代码。 每个任务都有自己的 train.py 用于指定参数解析器、设置模型等，而 task.py 会覆盖 general/task.py 中的模板。 `eval.py` 文件应该有一个训练模型的检查点目录作为输入，并且应该在测试数据集上评估这个模型。 最后，文件“dataset.py”包含设置数据集所需的所有部分。\n",
    "* 请注意，此模板假定您可能有多个不同的任务和多个不同的模型。 如果您有更简单的设置，则可以固有地将模板缩小到一起。\n",
    "\n",
    "\n",
    "### 参数解析器\n",
    "\n",
    "* 通过参数解析器指定超参数是一种很好的做法。 参数解析器允许您调用类似 `python train.py --learning ... --seed ... --hidden_size ...` 等的方式进行训练。\n",
    "* 如果您有多个模型可供选择，您将拥有多组超参数。 可以在 [PyTorch Lightning 文档](https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html#argparser-best-practices) 中找到一个很好的总结，而无需使用 Lightning。 本质上，您可以为每个模型定义一个静态方法，该方法返回一个针对其特定超参数的解析器。 这使您的代码更清晰，更容易定义新任务，而无需复制整个参数解析器。\n",
    "* 为确保可重复性（下面有更多详细信息），建议将参数保存为 json 文件或检查点文件夹中的类似文件。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数搜索\n",
    "\n",
    "* 一般来说，超参数搜索都是靠经验。 一旦你训练了很多模型，你就会更容易选择合理的初步猜测超参数。\n",
    "* 第一种方法是查看与你的模型相关的工作，看看其他人使用了什么作为类似模型的超参数。 这将帮助您从一个合理的选择开始。\n",
    "* 超参数搜索可能很昂贵。 因此，在扩大它们之前，先尝试在浅层模型上进行搜索。\n",
    "* 虽然大型网格搜索是从模型中获得最佳效果的最佳方式，但运行起来通常是不合理的。 尝试对超参数进行分组，并逐个优化每组。\n",
    "\n",
    "### 工具包\n",
    "\n",
    "* PyTorch Lightning 提供了很多关于超参数搜索的有用技巧和工具包，例如：\n",
    "     * [学习率查找器](https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html) 绘制了几个初始批次的学习率与损失的关系图，并帮助您选择合理的学习率。\n",
    "     * [Autoscaling batch sizes](https://pytorch-lightning.readthedocs.io/en/latest/training_tricks.html#auto-scaling-of-batch-size) 找到给定 GPU 的最大可能的批处理大小（如果有帮助 你有非常深的大模型，很明显你需要尽可能大的批量）。\n",
    "* 为了比较多个超参数配置，您可以将它们添加到 TensorBoard。 这是比较多次运行的一种简洁方法。 如果有兴趣，可以在 [此处](https://towardsdatascience.com/a-complete-guide-to-using-tensorboard-with-pytorch-53cb2301e8c3) 找到关于此的博客。\n",
    "* 有多个库支持您进行自动超参数搜索。 可以在 [此处](https://medium.com/pytorch/accelerate-your-hyperparameter-optimization-with-pytorchs-ecosystem-tools-bc17001b9a49) 找到 PyTorch 中的一个很好的概述。\n",
    "\n",
    "### 可重复性\n",
    "\n",
    "* 一切都与可重复性有关。 确保您可以使用相同的随机值、批次等重现您所做的任何训练。您会遇到这样一种情况，您已经尝试了很多不同的方法，但没有一种方法能够改进您之前的一次运行。 当您尝试使用最佳超参数再次运行模型时，您不希望有一个糟糕的惊喜（相信我，有足够多的人遇到这个问题，而且它也可能发生在您身上）。 因此，在开始任何网格搜索之前，请确保您能够重现运行。 使用相同的超参数、种子等在 Lisa 上并行运行两个作业，如果您没有得到完全相同的结果，请先停止并尝试修复它。\n",
    "* 关于再现性的另一个事实是保存和加载模型没有任何问题。 在长时间训练之前，请确保您能够从磁盘加载保存的模型，并获得与训练期间完全相同的测试分数。\n",
    "* 将您的超参数打印到 SLURM 输出文件中（python 中的简单打印语句）。 这将帮助您识别运行，并且您可以轻松检查 Lisa 是否执行了您打算执行的作业。 此外，超参数应存储在检查点目录中的单独文件中，无论是由 PyTorch Lightning 还是您自己保存。\n",
    "* 运行作业时，自动将作业文件复制到您的检查点文件夹。 这通过确保您准备好准确的运行评论来提高可重复性。\n",
    "* 除了 slurm 输出文件之外，创建一个输出文件，您可以在其中存储最佳训练、验证和测试分数。 当您想要快速比较多个模型或创建结果统计信息时，这会有所帮助。\n",
    "* 如果您想安全起见并使用 git，您甚至可以打印/保存您当前所在的 git 提交的哈希值，以及您对文件所做的任何更改。 可以在 [此处](https://github.com/Nithin-Holla/meme_challenge/blob/f4dc2079acb78ae30caaa31e112c4c210f93bf27/utils/save.py#L26) 找到如何执行此操作的示例。\n",
    "\n",
    "### 种子\n",
    "\n",
    "* DL 模型本质上是随机嘈杂的，如果您不进行确定性执行，则没有两次运行是相同的。 在运行网格搜索之前，请尝试了解您的实验可能有多嘈杂。 与结果比例相比，您期望的噪声越多，您需要运行的模型版本越多，才能在设置之间获得统计上的显着差异。\n",
    "* 完成网格搜索后，使用新种子运行另一个最佳配置模型。 如果分数仍然是最好的，就拿模型。 如果没有，请考虑为网格搜索中的前 $k$ 模型运行更多种子。 否则，您可能会采用次优模型，而该模型很幸运是特定种子的最佳模型。\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "* The learning rate is an important parameter, which depends on the optimizer, the model, and many more other hyperparameters.\n",
    "* A usual good starting point is 0.1 for SGD, and 1e-3 for Adam.\n",
    "* The deeper the model is, the lower the learning rate usually should be. For instance, Transformer models usually apply learning rates of 1e-5 to 1e-4 for Adam.\n",
    "* The lower your batch, the lower the learning rate should be. Consider using [gradient accumulation](https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa) if your batch size is getting too small (PyTorch Lightning supports this, see [here](https://pytorch-lightning.readthedocs.io/en/latest/training_tricks.html#accumulate-gradients)). \n",
    "* Consider using the PyTorch Lightning [learning rate finder](https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html) toolkit for an initial good guess. \n",
    "\n",
    "#### LR scheduler\n",
    "\n",
    "* Similarly to the learning rate, the scheduler to apply again depends on the classifier and model.\n",
    "* For image classifiers and SGD as optimizer, the multi-step LR scheduler has shown to be good choice.\n",
    "* Models trained with Adam commonly use a smooth exponential decay in the learning rate or a cosine-like scheduler.\n",
    "* For Transformers: remember to use a learning rate warmup. The cosine scheduler is often used for decaying the learning rate afterwards, but can also be replaced by an exponential decay.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "* Regularization is important in networks if you see a significantly higher training performance than test performance.\n",
    "* The regularization parameters all interact with each other, and hence must be tuned together. The most commonly used regularization techniques are: \n",
    "    * Weight decay\n",
    "    * Dropout\n",
    "    * Augmentation\n",
    "* Dropout is usually a good idea as it is applicable to most architectures and has shown to effectively reduce overfitting.\n",
    "* If you want to use weight decay in Adam, remember to use `torch.optim.AdamW` instead of `torch.optim.Adam`.\n",
    "\n",
    "#### Domain specific regularization\n",
    "\n",
    "* There are couple of regularization techniques that depend on your input data/domain. The most common include:\n",
    "    * Computer Vision: image augmentation like horizontal flip, rotation, scale-and-crop, color distortion, gaussian noise, etc.\n",
    "    * NLP: input dropout of whole words.\n",
    "    * Graphs: dropping edges, nodes, or part of the features of all nodes.\n",
    "\n",
    "\n",
    "### Grid search with SLURM \n",
    "\n",
    "* SLURM supports you to do a grid search with [job arrays](https://help.rc.ufl.edu/doc/SLURM_Job_Arrays). We have discussed job arrays in the [Lisa guide](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial1/Lisa_Cluster.html#Job-Arrays).\n",
    "* Job arrays allow you to start N jobs in parallel, each running with slightly different settings.\n",
    "* It is effectively the same as creating N job files and calling N times `sbatch ...`, but this can become annoying and is messy at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning\n",
    "\n",
    "Writing the job arrays can be sometimes annoying, and hence it is adviced to write a script that can automatically generate the hyperparameter files if you have to do this often enough (for instance, by adding the seed parameter 4 times to each other hyperparam config). However, if you are using PyTorch Lightning, you can directly create a job array file. The documentation for this can be found [here](https://pytorch-lightning.readthedocs.io/en/latest/slurm.html#building-slurm-scripts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
    "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
